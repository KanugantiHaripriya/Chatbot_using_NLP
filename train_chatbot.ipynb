{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is NLP?\n",
    "\n",
    "Natural Language Processing (NLP) is a field in Computer Science and Artificial Intelligence (AI) that enables machines to understand, interpret, and respond to human language in a meaningful way. It focuses on bridging the gap between human communication and computer understanding, allowing for tasks such as language translation, sentiment analysis, text summarization, and more.\n",
    "\n",
    "### Popular NLP Libraries:\n",
    "- **Spacy**: A fast and efficient library for NLP tasks such as tokenization, part-of-speech tagging, named entity recognition, etc.\n",
    "- **Gensim**: Primarily used for topic modeling, document similarity, and word embedding (Word2Vec, etc.).\n",
    "- **NLTK (Natural Language Toolkit)**: A powerful library for text processing, including classification, tokenization, stemming, and more.\n",
    "- **Scikit-Learn**: A general-purpose machine learning library that can be applied to NLP for tasks like classification and clustering.\n",
    "- **TensorFlow**: A deep learning framework often used for building advanced NLP models, such as neural networks for text generation or classification.\n",
    "- **Hugging Face**: A library that provides pre-trained NLP models, especially for transformer-based architectures like BERT, GPT, etc., for state-of-the-art language understanding.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation of Chatbot Using NLP\n",
    "\n",
    "NLP enables chatbots to understand, process, and respond to human language through tokenization, lemmatization, and intent recognition. The process involves collecting and preprocessing data, training a model to classify user intents, and generating appropriate responses. Chatbots are built by defining intents, vectorizing data, and integrating frontend and backend for real-time interaction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Libraries\n",
    "\n",
    "In this block, we import the necessary libraries for the chatbot project:\n",
    "\n",
    "- **nltk**: Used for natural language processing (NLP), particularly for tokenization.\n",
    "- **numpy**: Used for numerical operations, particularly in array manipulation.\n",
    "- **tensorflow**: The main machine learning library used for building and training the neural network.\n",
    "- **json**: For working with the intents dataset in JSON format.\n",
    "- **random**: For selecting random responses from the intents data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Hari\n",
      "[nltk_data]     Priya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import json\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Intents Dataset\n",
    "\n",
    "In this block, we open and load the **intents.json** file, which contains the dataset used for training the chatbot. The file is loaded using Python's built-in **json** module, and its content is stored in the `intents` variable as a Python dictionary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intents.json') as file:\n",
    "    intents = json.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the Data and Saving Words and Classes\n",
    "\n",
    "In this block, we tokenize the patterns from the intents dataset, extract unique words and classes, and save them as **pickle files** for later use. The steps involved are as follows:\n",
    "\n",
    "1. **Tokenization and Preprocessing**: Each sentence in the dataset is tokenized into individual words, which are then stored in the `words` list. Additionally, each pattern is paired with its corresponding tag (class) and added to the `documents` list.\n",
    "2. **Removing Stop Words**: Common punctuation marks such as `?`, `!`, `.`, and others are excluded from the `words` list.\n",
    "3. **Stemming and Lowercasing**: All words are converted to lowercase to ensure uniformity, and duplicate words are removed.\n",
    "4. **Saving Data**: The list of unique words and classes (tags) are saved as **pickle** files, allowing them to be reused during model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words.pkl and classes.pkl saved!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "# Initialize lists\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?', '!', '.', ',', ':', '(', ')']\n",
    "\n",
    "# Tokenize and preprocess the dataset\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        word_list = nltk.word_tokenize(pattern)\n",
    "        words.extend(word_list)\n",
    "        documents.append((pattern, intent['tag']))\n",
    "    if intent['tag'] not in classes:\n",
    "        classes.append(intent['tag'])\n",
    "\n",
    "# Stem and lower each word\n",
    "words = [w.lower() for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))  # Remove duplicates\n",
    "\n",
    "# Sort classes\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "# Save the words and classes as pickle files\n",
    "with open('words.pkl', 'wb') as f:\n",
    "    pickle.dump(words, f)\n",
    "\n",
    "with open('classes.pkl', 'wb') as f:\n",
    "    pickle.dump(classes, f)\n",
    "\n",
    "# Print confirmation message\n",
    "print(\"words.pkl and classes.pkl saved!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Training Data and Feature Extraction\n",
    "\n",
    "In this block, we prepare the training data and convert it into a format suitable for training the neural network. The key steps are as follows:\n",
    "\n",
    "1. **Preprocessing Sentences**: We tokenize each sentence from the `documents` list and remove the stop words. The resulting tokenized words are stored in the `training_sentences` list.\n",
    "2. **Labeling Sentences**: Each sentence is paired with its corresponding intent label, which is mapped to a unique index from the `classes` list and stored in `training_labels`.\n",
    "3. **One-Hot Encoding of Labels**: The `training_labels` are converted into one-hot encoded vectors using `tf.keras.utils.to_categorical()`. This converts each label into a binary vector representing the class.\n",
    "4. **Bag of Words Model**: The `CountVectorizer` from `sklearn` is used to convert the sentences into a bag of words model, representing each sentence as a feature vector. The vocabulary for the vectorizer is derived from the unique words stored in `words`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize training data\n",
    "training_sentences = []\n",
    "training_labels = []\n",
    "\n",
    "# Create the training set\n",
    "for doc in documents:\n",
    "    pattern_words = nltk.word_tokenize(doc[0])\n",
    "    pattern_words = [word.lower() for word in pattern_words if word not in ignore_words]\n",
    "    \n",
    "    training_sentences.append(\" \".join(pattern_words))\n",
    "    \n",
    "    # Set the label (the intent)\n",
    "    training_labels.append(classes.index(doc[1]))\n",
    "\n",
    "# Convert training labels to one-hot encoding\n",
    "training_labels = tf.keras.utils.to_categorical(training_labels, num_classes=len(classes))\n",
    "\n",
    "# Create the bag of words model (input feature vector)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(vocabulary=words)\n",
    "X_train = vectorizer.transform(training_sentences).toarray()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing and Compiling the Neural Network Model\n",
    "\n",
    "In this block, we define and compile a simple feedforward neural network model for intent classification:\n",
    "\n",
    "1. **Input Layer**: We add a dense layer with 128 units and the ReLU activation function. The input shape is defined based on the length of the feature vector (`len(X_train[0])`), which corresponds to the number of words in the vocabulary.\n",
    "2. **Dropout Layer**: A dropout layer is added with a rate of 0.5 to reduce overfitting during training.\n",
    "3. **Output Layer**: The output layer consists of a dense layer with a number of units equal to the number of classes (intents). The activation function is softmax, which is suitable for multi-class classification as it outputs probabilities.\n",
    "4. **Compiling the Model**: The model is compiled using the categorical cross-entropy loss function, the SGD optimizer with a learning rate of 0.01 and momentum of 0.9, and the accuracy metric to monitor the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Hari Priya\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the neural network model\n",
    "model = Sequential()\n",
    "\n",
    "# Add input layer\n",
    "model.add(Dense(128, input_shape=(len(X_train[0]),), activation='relu'))\n",
    "\n",
    "# Add dropout for regularization\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Add output layer with softmax activation to output probabilities\n",
    "model.add(Dense(len(classes), activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer=SGD(learning_rate=0.01, momentum=0.9), metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this block, we train the neural network model with the training data:\n",
    "\n",
    "1. **Training the Model**: The model is trained using the `fit()` function, where:\n",
    "   - `X_train` is the feature matrix containing the bag of words for each sentence.\n",
    "   - `training_labels` is the one-hot encoded vector representing the corresponding intent.\n",
    "   - The model is trained for 100 epochs with a batch size of 8.\n",
    "   - The `verbose=1` argument ensures that progress updates are printed during training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.0095 - loss: 5.6150\n",
      "Epoch 2/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0113 - loss: 5.5944\n",
      "Epoch 3/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0267 - loss: 5.5725\n",
      "Epoch 4/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0244 - loss: 5.5456\n",
      "Epoch 5/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0457 - loss: 5.5184\n",
      "Epoch 6/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0360 - loss: 5.4875\n",
      "Epoch 7/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0333 - loss: 5.4517\n",
      "Epoch 8/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0452 - loss: 5.4144\n",
      "Epoch 9/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0458 - loss: 5.3704\n",
      "Epoch 10/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0520 - loss: 5.3383\n",
      "Epoch 11/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0669 - loss: 5.2691\n",
      "Epoch 12/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0813 - loss: 5.2020\n",
      "Epoch 13/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.0902 - loss: 5.1336\n",
      "Epoch 14/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1007 - loss: 5.0278\n",
      "Epoch 15/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1222 - loss: 4.9400\n",
      "Epoch 16/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1378 - loss: 4.8981\n",
      "Epoch 17/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1526 - loss: 4.7659\n",
      "Epoch 18/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.1997 - loss: 4.7226\n",
      "Epoch 19/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2139 - loss: 4.5591\n",
      "Epoch 20/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2456 - loss: 4.4392\n",
      "Epoch 21/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2629 - loss: 4.3720\n",
      "Epoch 22/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2985 - loss: 4.2127\n",
      "Epoch 23/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.2982 - loss: 4.1040\n",
      "Epoch 24/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3297 - loss: 4.0237\n",
      "Epoch 25/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3174 - loss: 3.8825\n",
      "Epoch 26/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3461 - loss: 3.7881\n",
      "Epoch 27/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.3826 - loss: 3.6326\n",
      "Epoch 28/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4002 - loss: 3.5101\n",
      "Epoch 29/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4493 - loss: 3.3037\n",
      "Epoch 30/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4758 - loss: 3.1635\n",
      "Epoch 31/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4914 - loss: 3.0754\n",
      "Epoch 32/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.4800 - loss: 3.0365\n",
      "Epoch 33/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5078 - loss: 2.8983\n",
      "Epoch 34/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5593 - loss: 2.7469\n",
      "Epoch 35/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5937 - loss: 2.6812\n",
      "Epoch 36/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6029 - loss: 2.5660\n",
      "Epoch 37/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6035 - loss: 2.3357\n",
      "Epoch 38/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6444 - loss: 2.3301\n",
      "Epoch 39/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6391 - loss: 2.1953\n",
      "Epoch 40/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6926 - loss: 2.0377\n",
      "Epoch 41/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6854 - loss: 2.0062\n",
      "Epoch 42/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7196 - loss: 1.9172\n",
      "Epoch 43/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7174 - loss: 1.8435\n",
      "Epoch 44/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7521 - loss: 1.7008\n",
      "Epoch 45/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7586 - loss: 1.6627\n",
      "Epoch 46/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7602 - loss: 1.6241\n",
      "Epoch 47/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8164 - loss: 1.4252\n",
      "Epoch 48/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7907 - loss: 1.3878\n",
      "Epoch 49/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8215 - loss: 1.3016\n",
      "Epoch 50/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7777 - loss: 1.3473\n",
      "Epoch 51/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8139 - loss: 1.2681\n",
      "Epoch 52/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8433 - loss: 1.1344\n",
      "Epoch 53/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8054 - loss: 1.1525\n",
      "Epoch 54/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8416 - loss: 1.1359\n",
      "Epoch 55/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8562 - loss: 1.0437\n",
      "Epoch 56/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8575 - loss: 1.0798\n",
      "Epoch 57/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8804 - loss: 0.9165\n",
      "Epoch 58/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8839 - loss: 0.9266\n",
      "Epoch 59/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8629 - loss: 0.9161\n",
      "Epoch 60/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8519 - loss: 0.8887\n",
      "Epoch 61/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8835 - loss: 0.8585\n",
      "Epoch 62/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8711 - loss: 0.8126\n",
      "Epoch 63/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8901 - loss: 0.7956\n",
      "Epoch 64/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9045 - loss: 0.7236\n",
      "Epoch 65/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9007 - loss: 0.7470\n",
      "Epoch 66/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9143 - loss: 0.6777\n",
      "Epoch 67/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9301 - loss: 0.6817\n",
      "Epoch 68/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9053 - loss: 0.6220\n",
      "Epoch 69/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9253 - loss: 0.6148\n",
      "Epoch 70/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9048 - loss: 0.6510\n",
      "Epoch 71/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9185 - loss: 0.6050\n",
      "Epoch 72/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9395 - loss: 0.5408\n",
      "Epoch 73/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9138 - loss: 0.5744\n",
      "Epoch 74/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9303 - loss: 0.5245\n",
      "Epoch 75/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9356 - loss: 0.5044\n",
      "Epoch 76/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9308 - loss: 0.5238\n",
      "Epoch 77/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9364 - loss: 0.4636\n",
      "Epoch 78/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9302 - loss: 0.4494\n",
      "Epoch 79/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9516 - loss: 0.4563\n",
      "Epoch 80/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9296 - loss: 0.4312\n",
      "Epoch 81/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9482 - loss: 0.4310\n",
      "Epoch 82/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9513 - loss: 0.4208\n",
      "Epoch 83/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9527 - loss: 0.4201\n",
      "Epoch 84/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9378 - loss: 0.4590\n",
      "Epoch 85/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9525 - loss: 0.3842\n",
      "Epoch 86/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9638 - loss: 0.3603\n",
      "Epoch 87/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9359 - loss: 0.3826\n",
      "Epoch 88/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9494 - loss: 0.3932\n",
      "Epoch 89/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9611 - loss: 0.4000\n",
      "Epoch 90/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9515 - loss: 0.3645\n",
      "Epoch 91/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9494 - loss: 0.3713\n",
      "Epoch 92/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9652 - loss: 0.3335\n",
      "Epoch 93/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9524 - loss: 0.3410\n",
      "Epoch 94/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9497 - loss: 0.3210\n",
      "Epoch 95/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9670 - loss: 0.3321\n",
      "Epoch 96/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9535 - loss: 0.3244\n",
      "Epoch 97/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9688 - loss: 0.3038\n",
      "Epoch 98/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9575 - loss: 0.3083\n",
      "Epoch 99/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9773 - loss: 0.2586\n",
      "Epoch 100/100\n",
      "\u001b[1m105/105\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9733 - loss: 0.3021\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "history = model.fit(X_train, training_labels, epochs=100, batch_size=8, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the Trained Model\n",
    "\n",
    "After training the model, we save it to a file so that it can be loaded later for making predictions. The model is saved in the HDF5 format using the `save()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the trained model\n",
    "model.save('chatbot_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Predict the Class of a Sentence\n",
    "\n",
    "This function predicts the class of a given sentence using the trained model. It tokenizes and processes the input sentence, converts it into a bag of words, and uses the model to predict the class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
      "Predicted class: goodbye\n"
     ]
    }
   ],
   "source": [
    "# Function to predict the class of a sentence\n",
    "def predict_class(sentence, model, words, classes):\n",
    "    # Tokenize and stem the input sentence\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words = [w.lower() for w in sentence_words if w in words]\n",
    "    \n",
    "    # Convert the sentence to a bag of words\n",
    "    bag_of_words = np.zeros(len(words), dtype=int)\n",
    "    for word in sentence_words:\n",
    "        bag_of_words[words.index(word)] = 1\n",
    "\n",
    "    # Predict the class\n",
    "    prediction = model.predict(np.array([bag_of_words]))[0]\n",
    "    predicted_class_index = np.argmax(prediction)\n",
    "    return classes[predicted_class_index]\n",
    "\n",
    "# Example: Predict a sentence\n",
    "sentence = \"Bye\"\n",
    "predicted_class = predict_class(sentence, model, words, classes)\n",
    "print(f\"Predicted class: {predicted_class}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to Get a Response Based on the Predicted Class\n",
    "\n",
    "This function retrieves a response from the `intents.json` file based on the predicted class. It selects a random response associated with the predicted intent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: See you later\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Function to get the response based on the predicted class\n",
    "def get_response(predicted_class, intents):\n",
    "    for intent in intents['intents']:\n",
    "        if intent['tag'] == predicted_class:\n",
    "            response = random.choice(intent['responses'])\n",
    "            return response\n",
    "\n",
    "# Get the response\n",
    "response = get_response(predicted_class, intents)\n",
    "print(f\"Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Chatbot with a Sentence\n",
    "\n",
    "This code tests the chatbot by predicting the class for a given sentence (\"Bye\") and fetching a response based on the predicted class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 89ms/step\n",
      "Response: I'm fine, thank you\n"
     ]
    }
   ],
   "source": [
    "# Test with a sentence\n",
    "sentence = \"hey whats up\"\n",
    "predicted_class = predict_class(sentence, model, words, classes)\n",
    "response = get_response(predicted_class, intents)\n",
    "print(f\"Response: {response}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
